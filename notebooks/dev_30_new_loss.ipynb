{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 09:52:07,454 - matplotlib.pyplot - DEBUG - Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictors\n",
    "X_raw = np.random.random(100*9)\n",
    "X_raw = np.reshape(X_raw, (100, 9))\n",
    "\n",
    "# Standardize the predictors\n",
    "scaler = StandardScaler().fit(X_raw)\n",
    "X = scaler.transform(X_raw)\n",
    "\n",
    "# Add an intercept column to the model.\n",
    "X = np.abs(np.concatenate((np.ones((X.shape[0],1)), X), axis=1))\n",
    "\n",
    "# Define my \"true\" beta coefficients\n",
    "beta = np.array([2,6,7,3,5,7,1,2,2,8])\n",
    "\n",
    "# Y = Xb\n",
    "Y_true = np.matmul(X,beta)\n",
    "\n",
    "# Observed data with noise\n",
    "Y = Y_true*np.exp(np.random.normal(loc=0.0, scale=0.2, size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (Y.all()>=0 & (Y_true.all()>=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_root_error(y_pred, y_true, sample_weights=None):\n",
    "\n",
    "    y_true_sqrt = np.sqrt(np.array(y_true))\n",
    "    y_pred_sqrt = np.sqrt(np.array(y_pred))\n",
    "    #print('Failed.', y_true, y_pred)\n",
    "    assert len(y_true_sqrt) == len(y_pred_sqrt)\n",
    "        \n",
    "    if type(sample_weights) == type(None):\n",
    "        return(np.mean(np.square(y_true_sqrt - y_pred_sqrt)))\n",
    "    else:\n",
    "        sample_weights = np.array(sample_weights)\n",
    "        assert len(sample_weights) == len(y_true_sqrt)\n",
    "        return(np.dot(sample_weights, (np.square(y_true_sqrt - y_pred_sqrt)))/sum(sample_weights))\n",
    "\n",
    "def MSRE_loss(beta, X, Y, sample_weights):\n",
    "    return mean_square_root_error(np.matmul(X,beta), Y, sample_weights=sample_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearModel:\n",
    "    \"\"\"\n",
    "    Linear model: Y = XB, fit by minimizing the provided loss_function\n",
    "    with L1 regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 loss_function=None, \n",
    "                 X=None, \n",
    "                 Y=None, \n",
    "                 sample_weights=None, \n",
    "                 beta_init=None, \n",
    "                 regularization=0.00012):\n",
    "        self.regularization = regularization\n",
    "        self.beta = None\n",
    "        self.residue_loss = loss_function\n",
    "        self.sample_weights = sample_weights\n",
    "        self.beta_init = beta_init\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = np.matmul(X, self.beta)\n",
    "        return(prediction)\n",
    "\n",
    "    def model_error(self):\n",
    "        error = self.residue_loss(\n",
    "            self.predict(self.X), self.Y, sample_weights=self.sample_weights\n",
    "        )\n",
    "        return(error)\n",
    "    \n",
    "    def l1_regularized_loss(self, beta):\n",
    "        self.beta = beta\n",
    "        return(self.model_error() + \\\n",
    "               sum(self.regularization*np.array(self.beta)))\n",
    "    \n",
    "    def l2_regularized_loss(self, beta):\n",
    "        self.beta = beta\n",
    "        return(self.model_error() + \\\n",
    "               sum(self.regularization*np.array(self.beta)**2))\n",
    "    \n",
    "    def fit(self, maxiter=250):        \n",
    "        if type(self.beta_init)==type(None):\n",
    "            self.beta_init = np.array([1]*self.X.shape[1]) # default init: beta = 1\n",
    "        else: \n",
    "            pass\n",
    "            \n",
    "        if self.beta!=None and all(self.beta_init == self.beta):\n",
    "            logging.info(\"Model already fit once; continuing fit with more itrations.\")\n",
    "            \n",
    "        res = minimize(self.l1_regularized_loss, self.beta_init,\n",
    "                       method='TNC', options={'maxiter': maxiter},\n",
    "                       bounds=[(0, None) for n in range(X.shape[1])])\n",
    "        self.beta = res.x\n",
    "        self.beta_init = self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6315/4098005280.py:52: OptimizeWarning: Unknown solver options: maxiter\n",
      "  res = minimize(self.l1_regularized_loss, self.beta_init,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.77242623, 7.0151385 , 5.69839063, 2.91421513, 3.02609171,\n",
       "       5.03206811, 0.8422265 , 3.63434853, 0.10349791, 6.60342754])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_msre_model = CustomLinearModel(\n",
    "    loss_function=mean_square_root_error,\n",
    "    X=X, Y=Y, regularization=0.01\n",
    ")\n",
    "l2_msre_model.fit()\n",
    "l2_msre_model.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'optimization.inference' from '/mnt/cmnfs/proj/ORIGINS/protMSD/maxquant/ScanByScan/optimization/inference.py'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([6.85834358e+11, 1.06059742e+12, 3.99403137e+11, 1.12412418e+12,\n",
       "       1.26757140e+12, 5.91476871e+11, 3.28156038e+11, 2.57700513e+11,\n",
       "       8.91142371e+08, 2.66637032e+11])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optimization.inference\n",
    "reload(optimization.inference)\n",
    "from optimization.inference import CustomLinearModel\n",
    "# Generate predictors\n",
    "X_raw = np.random.random(100*9)\n",
    "X_raw = np.reshape(X_raw, (100, 9))\n",
    "\n",
    "# Standardize the predictors\n",
    "scaler = StandardScaler().fit(X_raw)\n",
    "X = scaler.transform(X_raw)\n",
    "\n",
    "# Add an intercept column to the model.\n",
    "X = np.abs(np.concatenate((np.ones((X.shape[0],1)), X), axis=1))\n",
    "\n",
    "# Define my \"true\" beta coefficients\n",
    "beta = np.array([26941161651,0,7545643210651,3489484,5154981321,7516613132,0,561648132132,2,1115158])\n",
    "\n",
    "# Y = Xb\n",
    "Y_true = np.matmul(X,beta)\n",
    "\n",
    "# Observed data with noise\n",
    "Y = Y_true*np.exp(np.random.normal(loc=0.0, scale=0.2, size=100))\n",
    "\n",
    "sol = CustomLinearModel(residue_loss=mean_square_root_error,\n",
    "                                    X=X,\n",
    "                                    Y=Y_true,            \n",
    "                                    reg_norm='l1',\n",
    "                                    reg_param=0.1)\n",
    "sol.fit(maxiter=15000,\n",
    "        method='Nelder-Mead')\n",
    "sol.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.78873744e+10, 0.00000000e+00, 7.54641791e+12, 1.99925088e+09,\n",
       "        7.54811658e+09, 1.04636814e+10, 2.47563671e+09, 5.61386594e+11,\n",
       "        1.42848537e+08, 0.00000000e+00]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import sparse_encode\n",
    "beta = sparse_encode(X = Y_true.reshape(-1, 1).T, \n",
    "                dictionary=X.T, \n",
    "                algorithm='lasso_cd',\n",
    "                positive=True,  \n",
    "                alpha=0.1,\n",
    "                max_iter=250\n",
    "                #n_jobs=-1\n",
    "                )\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_beta</th>\n",
       "      <th>regularized_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4.772426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>7.015139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5.698391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.914215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.026092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>5.032068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.842227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>3.634349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.103498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>6.603428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_beta  regularized_beta\n",
       "0          2          4.772426\n",
       "1          6          7.015139\n",
       "2          7          5.698391\n",
       "3          3          2.914215\n",
       "4          5          3.026092\n",
       "5          7          5.032068\n",
       "6          1          0.842227\n",
       "7          2          3.634349\n",
       "8          2          0.103498\n",
       "9          8          6.603428"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"true_beta\": beta, \n",
    "    \"regularized_beta\": l2_msre_model.beta\n",
    "})[[\"true_beta\",  \"regularized_beta\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Used to cross-validate models and identify optimal lambda\n",
    "class CustomCrossValidator:\n",
    "    \n",
    "    \"\"\"\n",
    "    Cross validates arbitrary model using MAPE criterion on\n",
    "    list of lambdas.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, ModelClass,\n",
    "                 sample_weights=None,\n",
    "                 loss_function=MSRE_loss):\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.ModelClass = ModelClass\n",
    "        self.loss_function = loss_function\n",
    "        self.sample_weights = sample_weights\n",
    "    \n",
    "    def cross_validate(self, lambdas, num_folds=10):\n",
    "        \"\"\"\n",
    "        lambdas: set of regularization parameters to try\n",
    "        num_folds: number of folds to cross-validate against\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lambdas = lambdas\n",
    "        self.cv_scores = []\n",
    "        X = self.X\n",
    "        Y = self.Y \n",
    "        \n",
    "        # Beta values are not likely to differ dramatically\n",
    "        # between differnt folds. Keeping track of the estimated\n",
    "        # beta coefficients and passing them as starting values\n",
    "        # to the .fit() operator on our model class can significantly\n",
    "        # lower the time it takes for the minimize() function to run\n",
    "        beta_init = None\n",
    "        \n",
    "        for lam in self.lambdas:\n",
    "            print(\"Lambda: {}\".format(lam))\n",
    "            \n",
    "            # Split data into training/holdout sets\n",
    "            kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "            kf.get_n_splits(X)\n",
    "            \n",
    "            # Keep track of the error for each holdout fold\n",
    "            k_fold_scores = []\n",
    "            \n",
    "            # Iterate over folds, using k-1 folds for training\n",
    "            # and the k-th fold for validation\n",
    "            f = 1\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                # Training data\n",
    "                CV_X = X[train_index,:]\n",
    "                CV_Y = Y[train_index]\n",
    "                CV_weights = None\n",
    "                if type(self.sample_weights) != type(None):\n",
    "                    CV_weights = self.sample_weights[train_index]\n",
    "                \n",
    "                # Holdout data\n",
    "                holdout_X = X[test_index,:]\n",
    "                holdout_Y = Y[test_index]\n",
    "                holdout_weights = None\n",
    "                if type(self.sample_weights) != type(None):\n",
    "                    holdout_weights = self.sample_weights[test_index]\n",
    "                \n",
    "                # Fit model to training sample\n",
    "                lambda_fold_model = self.ModelClass(\n",
    "                    regularization=lam,\n",
    "                    X=CV_X,\n",
    "                    Y=CV_Y,\n",
    "                    sample_weights=CV_weights,\n",
    "                    beta_init=beta_init,\n",
    "                    loss_function=self.loss_function\n",
    "                )\n",
    "                lambda_fold_model.fit()\n",
    "                \n",
    "                # Extract beta values to pass as beta_init \n",
    "                # to speed up estimation of the next fold\n",
    "                beta_init = lambda_fold_model.beta\n",
    "                \n",
    "                # Calculate holdout error\n",
    "                fold_preds = lambda_fold_model.predict(holdout_X)\n",
    "                fold_msre = MSRE_loss(\n",
    "                    beta=beta_init, \n",
    "                    X=holdout_X, \n",
    "                    Y=holdout_Y, \n",
    "                    sample_weights=holdout_weights\n",
    "                )\n",
    "                k_fold_scores.append(fold_msre)\n",
    "                print(\"Fold: {}. Error: {}\".format( f, fold_msre))\n",
    "                f += 1\n",
    "            \n",
    "            # Error associated with each lambda is the average\n",
    "            # of the errors across the k folds\n",
    "            lambda_scores = np.mean(k_fold_scores)\n",
    "            print(\"LAMBDA AVERAGE: {}\".format(lambda_scores))\n",
    "            self.cv_scores.append(lambda_scores)\n",
    "        \n",
    "        # Optimal lambda is that which minimizes the cross-validation error\n",
    "        self.lambda_star_index = np.argmin(self.cv_scores)\n",
    "        self.lambda_star = self.lambdas[self.lambda_star_index]\n",
    "        print(\"\\n\\n**OPTIMAL LAMBDA: {}**\".format(self.lambda_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18374/2567237379.py:54: OptimizeWarning: Unknown solver options: maxiter\n",
      "  res = minimize(self.l1_regularized_loss, self.beta_init,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0\n",
      "Fold: 1. Error: 0.24161468288513116\n",
      "Fold: 2. Error: 0.4440726849158496\n",
      "Fold: 3. Error: 0.39231035399740993\n",
      "Fold: 4. Error: 0.22418302737394336\n",
      "Fold: 5. Error: 0.36152233065575695\n",
      "LAMBDA AVERAGE: 0.3327406159656182\n",
      "Lambda: 1\n",
      "Fold: 1. Error: 7.813011419927335\n",
      "Fold: 2. Error: 10.291948606588054\n",
      "Fold: 3. Error: 9.998813189547157\n",
      "Fold: 4. Error: 8.908148936872646\n",
      "Fold: 5. Error: 10.942262793729359\n",
      "LAMBDA AVERAGE: 9.59083698933291\n",
      "Lambda: 0.1\n",
      "Fold: 1. Error: 0.9529363268791888\n",
      "Fold: 2. Error: 1.085995217827644\n",
      "Fold: 3. Error: 0.9023950876505091\n",
      "Fold: 4. Error: 0.6193653711344176\n",
      "Fold: 5. Error: 0.860909121362684\n",
      "LAMBDA AVERAGE: 0.8843202249708886\n",
      "Lambda: 0.01\n",
      "Fold: 1. Error: 0.5047102711225226\n",
      "Fold: 2. Error: 0.24133242861437054\n",
      "Fold: 3. Error: 0.31536502080120465\n",
      "Fold: 4. Error: 0.3147332698657366\n",
      "Fold: 5. Error: 0.42414583415011153\n",
      "LAMBDA AVERAGE: 0.36005736491078916\n",
      "Lambda: 0.001\n",
      "Fold: 1. Error: 0.32076192890783306\n",
      "Fold: 2. Error: 0.2937394958262522\n",
      "Fold: 3. Error: 0.33604966941245257\n",
      "Fold: 4. Error: 0.31028020079338975\n",
      "Fold: 5. Error: 0.41905479843846727\n",
      "LAMBDA AVERAGE: 0.335977218675679\n",
      "Lambda: 0.0001\n",
      "Fold: 1. Error: 0.37794109059844283\n",
      "Fold: 2. Error: 0.2747936140438624\n",
      "Fold: 3. Error: 0.5086634048073354\n",
      "Fold: 4. Error: 0.3982935675023251\n",
      "Fold: 5. Error: 0.21143121372126655\n",
      "LAMBDA AVERAGE: 0.3542245781346465\n",
      "Lambda: 1e-05\n",
      "Fold: 1. Error: 0.2636690068885768\n",
      "Fold: 2. Error: 0.45210406677860415\n",
      "Fold: 3. Error: 0.34038382642682596\n",
      "Fold: 4. Error: 0.2776076892430938\n",
      "Fold: 5. Error: 0.31840244207144497\n",
      "LAMBDA AVERAGE: 0.3304334062817092\n",
      "Lambda: 1e-06\n",
      "Fold: 1. Error: 0.46878132396144334\n",
      "Fold: 2. Error: 0.27046116994742275\n",
      "Fold: 3. Error: 0.33495317614112485\n",
      "Fold: 4. Error: 0.4144208152011529\n",
      "Fold: 5. Error: 0.21279934786544116\n",
      "LAMBDA AVERAGE: 0.34028316662331703\n",
      "\n",
      "\n",
      "**OPTIMAL LAMBDA: 1e-05**\n"
     ]
    }
   ],
   "source": [
    "# User must specify lambdas over which to search\n",
    "lambdas = [0, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "cross_validator = CustomCrossValidator(\n",
    "    X = X, Y = Y, \n",
    "    ModelClass=CustomLinearModel,\n",
    "    loss_function=mean_square_root_error\n",
    ")\n",
    "cross_validator.cross_validate(lambdas, num_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
